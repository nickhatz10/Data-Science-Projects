# -*- coding: utf-8 -*-
"""Midterm_Lasso.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ASCcX6nFiadnESxE0DaX_nI8V6YK3HO
"""

\# mount google drive to this notebook to be able to access files in my drive

from google.colab import drive
drive.mount('/content/drive')

# import statements that I will be using for data manipulation
import pandas as pd
import numpy as np

# create links to the csv files for the training and testing data
train_csv = '/content/drive/My Drive/Stat_380/Midterm/start_train.csv'
test_csv = '/content/drive/My Drive/Stat_380/Midterm/start_test.csv'
card_tab_csv = '/content/drive/My Drive/Stat_380/Midterm/card_tab.csv'
samp_sub_csv = '/content/drive/My Drive/Stat_380/Midterm/samp_sub.csv'
set_tab_csv = '/content/drive/My Drive/Stat_380/Midterm/set_tab.csv'

# use pandas to read the csv files and convert them to pandas dataframes
# give them corresponding variable names

train_data = pd.read_csv(train_csv)
test_data = pd.read_csv(test_csv)
card_tab = pd.read_csv(card_tab_csv)
set_tab = pd.read_csv(set_tab_csv)
samp_sub = pd.read_csv(samp_sub_csv)

# using the set_tab dataframe and choosing only the set and release date column from both
set_date = set_tab[["set", "release_date"]]

# changing the release date column values to datetime and then to numeric values so that 
# I can use them during machine learning
set_date['release_date'] = pd.to_numeric(pd.to_datetime(set_date['release_date']))

# merging the set tab dataframe with the card tab dataframe and joining the two df's on the set column
# this is a left join meaning we want to keep all of the values from the card tab df since this has the valuable data in it
# this will allow the card tab df to now have a date column which can help make the predictions more accurate later on
card_tab = card_tab.merge(set_date, on='set', how='left')

# replacing the values with asteriks in the power column to nan values

card_tab['power'] = card_tab['power'].replace('*', np.nan)
card_tab['power'] = card_tab['power'].replace('1+*', np.nan)

# replacing the values with asteriks in the toughness column to nan values

card_tab['toughness'] = card_tab['toughness'].replace('*', np.nan)
card_tab['toughness'] = card_tab['toughness'].replace('1+*', np.nan)

# replacing the values with X's in the loyalty column to nan values

card_tab['loyalty'] = card_tab['loyalty'].replace('X', np.nan)

# creates a function that will allow me to drop columns in the dataframe
# takes a dataframe and a list of the column names you want to drop as the input
def drop_column(df, lst):
  # for loop iterates through each column name in the list of column names
  for col in lst:
    # drops the specified column name from the dataframe
    df = df.drop(col, 1)
  #returns the new dataframe with the dropped columns
  return df

# REASONs WHY I DROPPED THE COLUMNS:
# I drop the card name column because it doesn't provide any useful information that will help predict the price of the card
# I drop the mana cost column since this doesn't contain numeric values and since the cmc column already has the converted mana cost given in int values
# I drop the type column because there are way too many type values and it would be too much to make dummy variables from
# I drop the subtypes column because there are way too many subtypes values and it would be too much to make dummy variables from
# I drop the set name column because it is just the full text name of the set column and it is easier to have the set column since it is the abbreviated name of the set_name column
# list of column names I want to drop from the dataframe
lst = ['card_name', "mana_cost", "type", "subtypes", "set_name"]

# new dataframe with the dropped columns
card_tab_drop = drop_column(card_tab, lst)

# maps numerical values to the ordinal data for rarity
# don't want to make this column into dummy variables since it is ordinal data and order matters
dict_map = {'Common':0,'Uncommon':1,'Rare':2,'Mythic':3}

# replaces each text value with the values assigned to them in the dictionary above
card_tab_drop["rarity"] = card_tab_drop["rarity"].replace(dict_map)

# creates dummy variables for all 5 colors
def label_color(df, color):
  #initializes an empty list
  lst = []
  # iterates through each value in the colors column
  for val in df['colors']:
    # uses a try statement just in case we receive an error
    try:
      # if the color we choose is in the colors value in the dataframe, then we append 1 to the list
      if color in val:
        lst.append(1)
        # if it isn't then we append 0
      else:
        lst.append(0)
    # if we receive a TypeError due to a nan value, then we append 0 to the list since this didn't have the color specified
    except TypeError:
      lst.append(0)
  # returns the list of binary values to make into a column
  return lst

# assigns each dummy variable list for each specific color to a variable identifying that colors name
green_dummy = label_color(card_tab_drop, "Green")
blue_dummy = label_color(card_tab_drop, "Blue")
black_dummy = label_color(card_tab_drop, "Black")
red_dummy = label_color(card_tab_drop, "Red")
white_dummy = label_color(card_tab_drop, "White")

# appends the colors dummy variable lists as columns in the df
card_tab_drop['green_dummy'] = green_dummy
card_tab_drop['blue_dummy'] = blue_dummy
card_tab_drop['black_dummy'] = black_dummy
card_tab_drop['red_dummy'] = red_dummy
card_tab_drop['white_dummy'] = white_dummy

# creates dummy variables for the different types
# same logic as in the above function
def label_types(df, type_name):
  lst = []
  for val in df['types']:
    try:
      if type_name in val:
        lst.append(1)
      else:
        lst.append(0)
    except TypeError:
      lst.append(0)
  return lst

# assigns each dummy variable list for each specific type name to a variable identifying that type name
ench_dummy = label_types(card_tab_drop, 'Enchantment')
creat_dummy = label_types(card_tab_drop, 'Creature')
land_dummy = label_types(card_tab_drop, 'Land')
inst_dummy = label_types(card_tab_drop, 'Instant')
sorc_dummy = label_types(card_tab_drop, 'Sorcery')
art_dummy = label_types(card_tab_drop, 'Artifact')
plane_dummy = label_types(card_tab_drop, 'Planeswalker')

# appends the types dummy variable lists to columns in the df
card_tab_drop['enchantment_dummy'] = ench_dummy
card_tab_drop['creature_dummy'] = creat_dummy
card_tab_drop['land_dummy'] = land_dummy
card_tab_drop['insant_dummy'] = inst_dummy
card_tab_drop['sorcery_dummy'] = sorc_dummy
card_tab_drop['artifact_dummy'] = art_dummy
card_tab_drop['planeswalker_dummy'] = plane_dummy

# creates a dummy variable for if the supertype column is legendary or not
# same logic as the function above
def label_types(df, supertype_name):
  lst = []
  for val in df['supertypes']:
    try:
      if supertype_name in val:
        lst.append(1)
      else:
        lst.append(0)
    except TypeError:
      lst.append(0)
  return lst

# creates the dummy variable to determine wether the supertype is legendary or not and assigns it to a variable name
legend_dummy = label_types(card_tab_drop, 'Legendary')

# appends the supertypes legendary dummy variable list to column in the df
card_tab_drop['legendary_dummy'] = legend_dummy

# create a novel binary feature for text that contains the word exile since this is a very useful/powerful action for a card
# same logic as the above function
def label_text(df):
  lst = []
  for text in df['text']:
    try:
      if "exile" in text.lower():
        lst.append(1)
      else:
        lst.append(0)
    except AttributeError:
      lst.append(0)
  return lst



# creates the dummy variable to determine wether the text contains the word exile or not and assigns it to a variable name
exile_dummy = label_text(card_tab_drop)

# appends the exile dummy variable list to a column in the df
card_tab_drop['exile_dummy'] = exile_dummy

# drops these 4 columns since we created dummy variable columns from them and don't need them anymore

card_tab_drop = card_tab_drop.drop('colors', axis = 1)

card_tab_drop = card_tab_drop.drop('types', axis = 1)

card_tab_drop = card_tab_drop.drop('supertypes', axis = 1)

card_tab_drop = card_tab_drop.drop('text', axis = 1)

# fills missing nan values with zeros

card_tab_drop['power'] = card_tab_drop['power'].replace(np.nan, 0)


card_tab_drop['toughness'] = card_tab_drop['toughness'].replace(np.nan, 0)


card_tab_drop['loyalty'] = card_tab_drop['loyalty'].replace(np.nan, 0)

# turns the set column into dummy variables
card_tab_drop = pd.get_dummies(card_tab_drop, columns=['set'])

# creates a novel feature that adds power and toughness together
card_tab_drop["power_plus_toughness"] = pd.to_numeric(card_tab_drop["power"]) + pd.to_numeric(card_tab_drop["toughness"])

# creates a novel feature that squares the value of loyalty
card_tab_drop["loyalty_squared"] = pd.to_numeric(card_tab_drop["loyalty"]) ** 2

# drop duplicates from the data and then merges the data on the id column of the train and test data
card_tab_drop = card_tab_drop.drop_duplicates()

# right merge makes sure that all the values from the train and test data are taken
card_tab_merged_train = card_tab_drop.merge(train_data, on='id', how='right')
card_tab_merged_test = card_tab_drop.merge(test_data, on='id', how='right')

# drops the id column from the test data since we will not be needing this when we make predictions 
card_tab_merged_test = card_tab_merged_test.drop('id', 1)

# assigns the future price column to a variable named train_labels
train_labels = card_tab_merged_train['future_price']

# drops the future price column and the id column from the train data since we won't be needing this
train = card_tab_merged_train.drop('future_price', 1)
train = train.drop('id', 1)

# uses sklearn Lasso regression
from sklearn.linear_model import Lasso
lasso_reg = Lasso()

# uses GridSearchCV to find the hyperparameters for the Lasso regression model that produce the best results
from sklearn.model_selection import GridSearchCV

# different hyperparameters that the grid search function will test
params_Lasso = {'alpha': [1,0.1,0.01,0.001,0.0001,0] , "fit_intercept": [True, False], "selection": ['cyclic', 'random']}

# uses cross validation of 5 to make sure the Lasso hyperparameters produce a great score among different folds of data
grid_search = GridSearchCV(lasso_reg, param_grid=params_Lasso, cv = 5, n_jobs=-1)

# after the grid search figures out which model is the best, it trains this model using the training data
# and then assigns this model to a variables named best_model  
best_model = grid_search.fit(train, train_labels)

# uses the best_model to predict on the test data and then assigns these predictions to a variable called y_pred
y_pred = best_model.predict(card_tab_merged_test)

# takes the predictions and puts them in the future price column of the sample submission dataframe
samp_sub['future_price'] = y_pred

# converts the sample submission dataframe with the predictions to a csv file that I will use to submit to kaggle
samp_sub.to_csv('/content/drive/My Drive/Stat_380/Midterm/submission9.csv', index = False, header=True)