# -*- coding: utf-8 -*-
"""HousePrice2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8dxSIyhKKzMV5DI7F8ah_kK55X0Pngq
"""

# mount google drive to this notebook to be able to access files in my drive

from google.colab import drive
drive.mount('/content/drive')

# import statements that I will be using for data manipulation
import pandas as pd
import numpy as np

# create links to the csv files for the training and testing data
train_csv = '/content/drive/My Drive/Stat_380/HousePrice2/train.csv'
test_csv = '/content/drive/My Drive/Stat_380/HousePrice2/test.csv'
samp_sub_csv = '/content/drive/My Drive/Stat_380/HousePrice2/sample_submission.csv'

# use pandas to read the csv files and convert them to pandas dataframes
# give them corresponding variable names

train_data = pd.read_csv(train_csv)
test_data = pd.read_csv(test_csv)
samp_sub = pd.read_csv(samp_sub_csv)

# turns the "BldgType" and 'Heating' columns into dummy variables for the train data
train_data = pd.get_dummies(train_data, columns=['BldgType', "Heating"])

# turns the "BldgType" and 'Heating' columns into dummy variables for the test data
test_data = pd.get_dummies(test_data, columns=['BldgType', "Heating"])

# makes the train data column values to binary: N = 0 and Y = 1
train_data['CentralAir'].replace('N', 0, inplace=True)
train_data['CentralAir'].replace('Y', 1, inplace=True)

# makes the test data column values to binary: N = 0 and Y = 1
test_data['CentralAir'].replace('N', 0, inplace=True)
test_data['CentralAir'].replace('Y', 1, inplace=True)

# creates a new column for total square foot area

train_data['total_square_ft'] = train_data['GrLivArea'] + train_data['TotalBsmtSF']
test_data['total_square_ft'] = test_data['GrLivArea'] + test_data['TotalBsmtSF']

# creates a new column for year sold minus year built

train_data['built_minus_sold'] = train_data['YrSold'] + train_data['YearBuilt']
test_data['built_minus_sold'] = test_data['YrSold'] + test_data['YearBuilt']

# train data
# drops the Id column since we won't need to use this for machine learning
# fills any NA or missing values in the table with the mean value of that column

train_data = train_data.drop("Id", 1)
train_data =  train_data.fillna(train_data.mean())

# test data
# drops the Id column since we won't need to use this for machine learning
# fills any NA or missing values in the table with the mean value of that column

test_data = test_data.drop("Id", 1)
test_data =  test_data.fillna(test_data.mean())

# assigns the labels (sale price) of the data to a variable named train_labels
# these labels will be used for training the data
train_labels = train_data['SalePrice']

# then deletes the sale price column since we already assigned it as the train labels variable
train_data = train_data.drop("SalePrice", 1)

# import statement for the gradient boosting algorithm which we will be using for prediction
from sklearn.ensemble import GradientBoostingRegressor

# uses Gradient Boosting to predict the sale price. Tuned the hyperparameters and found that n_estimators = 120
# and learning rate = 0.2 works the best and gives the lowest RMSE
gbr = GradientBoostingRegressor(n_estimators = 120, learning_rate=0.2)

# fits the data on the training data and training labels to train the algorithm for prediction
gbr.fit(train_data, train_labels)

# gets a list of all the column names in the training data
# this will be used for feature importance in the next cell
col_list = list(train_data.columns)

# import statements to graph a bar plot and also to use mathematical functions
from matplotlib import pyplot
import math

# gets the feature importances for each feature (column) in the training data
importance = gbr.feature_importances_

# loops through each feature using enumerate so that a number can be assigned to a feature
# prints the number of the feature, followed by the column name, and then lastly the feature importance score rounded to 4 decimal places
# this is important since it tells us which features correspond to which numbers
for i,v in enumerate(importance):
	print('Feature {}: {}, Score: {}'.format(i, col_list[i], round(v, 4)))
 
# plots the feature importance by creating a list using list comprehension that assigns each feature to a number
# a bar chart is plotted giving the number of the feature and then its score
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# the trained gradient boosting model is then given the test_data to make a prediction on
# the predictions for the test_data are then stored under the variable named predictions
predictions = gbr.predict(test_data)

# the sample submission "SalePrice column" is then replaced by the predictions of the test data
samp_sub['SalePrice'] = predictions

# this sample submission is then turned into a csv which I will submit to Kaggle for scoring
samp_sub.to_csv('/content/drive/My Drive/Stat_380/HousePrice2/submission4.csv', index = False, header=True)

# cross validation to see how accurate our model is among different folds
from sklearn.model_selection import cross_val_score
scores = cross_val_score(gbr, train_data, train_labels, cv=7, scoring='neg_root_mean_squared_error')

# prints out the cross validation score with the standard deviation rounded to 1 decimal place
# this shows us how accurate our model is at predicting the price over different folds of data

print("{} RMSE with a standard deviation of {}".format(round(scores.mean()*-1, 1), round(scores.std(), 1)))