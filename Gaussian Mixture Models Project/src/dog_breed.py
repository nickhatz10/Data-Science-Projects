# -*- coding: utf-8 -*-
"""Dog_Breed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFMbeWbBhLpi_DE_T-ifT4_WCCV5N9yM
"""

# mount google drive to this notebook to be able to access files in my drive

from google.colab import drive
drive.mount('/content/drive')

# import statements that I will be using for data manipulation
import pandas as pd
import numpy as np

# create links to the csv files for the training and testing data
data_csv = '/content/drive/My Drive/Stat_380/Dog Breeds/data.csv'
samp_sub_csv = '/content/drive/My Drive/Stat_380/Dog Breeds/Sample_submission.csv'

# use pandas to read the csv files and convert them to pandas dataframes
# give them corresponding variable names

data = pd.read_csv(data_csv)
samp_sub = pd.read_csv(samp_sub_csv)

# drop the ID column since we won't be needing this for machine learning
data = data.drop('id', 1)

# uses the Standard Scaler package from sklearn
# this will make the data have a mean of 0 and a standard deviation of 1
# this will help to account for the possibility of the different numbers in each
# column having different units and thus, the machine learning algorithm
# would be biased and treat one column with more importance over another
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# import the sklearn Guassian Mixture Model package

from sklearn.mixture import GaussianMixture

# set the parameters of the model
# n_components is how many clusters the model will create
# n_init deals with how many inilizations are performed, which can help prevent the model from converging on bad clusters
gm = GaussianMixture(n_components=4, n_init=25)

# after I initiliaze my model, I fit it to the scaled data
gm.fit(scaled_data)

# I then use the model to predict the outcomes of each sample
# this is necessary to match up the model's predictions for the breed cluster nummbers with what they actually are
# as provided for us on Kaggle to make sure the cluster numbers are correct before submission
gm.predict(scaled_data)

# this then predicts the probabilities of each sample being in all four clusters
probs = gm.predict_proba(scaled_data)

# I use pandas to switch up the cluster numbers for each prediction based on the ones given to us on Kaggle

import pandas as pd

df = pd.DataFrame(probs, columns = ['0', '1', '2', '3'])

df['id'] = samp_sub['id']

# this reorders the columns to fit the real column numbers
df = df[['3', '2', '0', '1', 'id']]

# this renames the columns to the correct Breed column header

df = df.rename(columns={"3": 'Breed1', '2': 'Breed2', '0': 'Breed3', '1': 'Breed4'})

# outputs the dataframe to make sure it worked correctly
df

sub1_csv = '/content/drive/My Drive/Stat_380/Dog Breeds/submission1.csv'

sub1_df = pd.read_csv(sub1_csv)

# converts the sample submission dataframe with the predictions to a csv file that I will use to submit to kaggle
df.to_csv('/content/drive/My Drive/Stat_380/Dog Breeds/submission.csv', index = False, header=True)