# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1If61cSXnEV25e9niGwxcopmpWoMfHVgo
"""

# mount google drive to acces files

from google.colab import drive
drive.mount('/content/drive')

# import statements that I will utilize
import pandas as pd
import numpy as np

# file path locations

train_csv = '/content/drive/My Drive/Stat_380/Final Project/train_embedded.csv'
test_csv = '/content/drive/My Drive/Stat_380/Final Project/test_embedded_final.csv'

train_reg_csv = '/content/drive/My Drive/Stat_380/Final Project/train_data_final.csv'

samp_sub_csv = '/content/drive/My Drive/Stat_380/Final Project/example_sub.csv'

# uses pandas read_csv() function to turn the csv files into a pandas dataframe

train_data = pd.read_csv(train_csv)
test_data = pd.read_csv(test_csv)

train_reg = pd.read_csv(train_reg_csv)

samp_sub = pd.read_csv(samp_sub_csv)

# uses standard scaler to scale the train and test data so that it has a mean of 0 and standard deviation of 1
# this is very important when performing dimension reduction as we don't want any one feature to have much more
# weight over another due to different units

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_train = scaler.fit_transform(train_data)
scaled_test = scaler.fit_transform(test_data)

# converts the scaled data to a dataframe and then combines the train and test to get it ready for pca
scaled_train = pd.DataFrame(scaled_train)
scaled_test = pd.DataFrame(scaled_test)


frames = [scaled_train, scaled_test]

scaled_combined = pd.concat(frames)

# performs PCA on the data and choosing the number of features that account for 95% of the variability
from sklearn.decomposition import PCA
pca = PCA(.95)
pca_scal_combine = pd.DataFrame(pca.fit_transform(scaled_combined))
pca.n_components_

# after we perform PCA, then run the reduced dimension data using TSNE, this will reduce it to 2 features. We perform multiple TSNE's with
# different perplexity's and then combine them all together to produce a better model


from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 30, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

# we then have to split up the data back into the train and test split
tsne_train1 = tsne_data[0:200]
tsne_test1 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 15, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train2 = tsne_data[0:200]
tsne_test2 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 65, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train3 = tsne_data[0:200]
tsne_test3 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 45, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train4 = tsne_data[0:200]
tsne_test4 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 55, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train5 = tsne_data[0:200]
tsne_test5 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 5, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train6 = tsne_data[0:200]
tsne_test6 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 75, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train7 = tsne_data[0:200]
tsne_test7 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 25, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train8 = tsne_data[0:200]
tsne_test8 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 10, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train9 = tsne_data[0:200]
tsne_test9 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 40, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train10 = tsne_data[0:200]
tsne_test10 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 28, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train11 = tsne_data[0:200]
tsne_test11 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 8, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train12 = tsne_data[0:200]
tsne_test12 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 49, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train13 = tsne_data[0:200]
tsne_test13 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 67, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train14 = tsne_data[0:200]
tsne_test14 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 32, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train15 = tsne_data[0:200]
tsne_test15 = tsne_data[200:]

from sklearn.manifold import TSNE

tsne_data = TSNE(n_components=2, perplexity = 49, learning_rate='auto', init='pca').fit_transform(pca_scal_combine)

tsne_train16 = tsne_data[0:200]
tsne_test16 = tsne_data[200:]

# combines all the train TSNE's together into one dataframe

tsne_train1 = pd.DataFrame(tsne_train1)
tsne_train2 = pd.DataFrame(tsne_train2)
tsne_train3 = pd.DataFrame(tsne_train3)
tsne_train4 = pd.DataFrame(tsne_train4)
tsne_train5 = pd.DataFrame(tsne_train5)
tsne_train6 = pd.DataFrame(tsne_train6)
tsne_train7 = pd.DataFrame(tsne_train7)
tsne_train8 = pd.DataFrame(tsne_train8)
tsne_train9 = pd.DataFrame(tsne_train9)
tsne_train10 = pd.DataFrame(tsne_train10)
tsne_train11 = pd.DataFrame(tsne_train11)
tsne_train12 = pd.DataFrame(tsne_train12)
tsne_train13 = pd.DataFrame(tsne_train13)
tsne_train14 = pd.DataFrame(tsne_train14)
tsne_train15 = pd.DataFrame(tsne_train15)
tsne_train16 = pd.DataFrame(tsne_train16)

train_frames = [tsne_train1, tsne_train2, tsne_train3, tsne_train4, tsne_train5, tsne_train6, tsne_train7, tsne_train8, tsne_train9, tsne_train10,
                tsne_train11, tsne_train12, tsne_train13, tsne_train14, tsne_train15, tsne_train16]

train_df = pd.concat(train_frames, axis=1)

train_df.columns = [x for x in range(32)]

# combines all of the test TSNE's data into one dataframe

tsne_test1 = pd.DataFrame(tsne_test1)
tsne_test2 = pd.DataFrame(tsne_test2)
tsne_test3 = pd.DataFrame(tsne_test3)
tsne_test4 = pd.DataFrame(tsne_test4)
tsne_test5 = pd.DataFrame(tsne_test5)
tsne_test6 = pd.DataFrame(tsne_test6)
tsne_test7 = pd.DataFrame(tsne_test7)
tsne_test8 = pd.DataFrame(tsne_test8)
tsne_test9 = pd.DataFrame(tsne_test9)
tsne_test10 = pd.DataFrame(tsne_test10)
tsne_test11 = pd.DataFrame(tsne_test11)
tsne_test12 = pd.DataFrame(tsne_test12)
tsne_test13 = pd.DataFrame(tsne_test13)
tsne_test14 = pd.DataFrame(tsne_test14)
tsne_test15 = pd.DataFrame(tsne_test15)
tsne_test16 = pd.DataFrame(tsne_test16)

test_frames = [tsne_test1, tsne_test2, tsne_test3, tsne_test4, tsne_test5, tsne_test6, tsne_test7, tsne_test8, tsne_test9, tsne_test10, 
               tsne_test11, tsne_test12, tsne_test13, tsne_test14, tsne_test15, tsne_test16]

test_df = pd.concat(test_frames, axis=1)

test_df.columns = [x for x in range(32)]

# relabeling the numbers in each of the dummy variable label columns
# this is important since we are going to combine all of these columns into one
# so we cannot have them all with 1 labels, they need to be different

train_reg['subredditCooking'] = train_reg['subredditCooking'].replace(1, 2)
train_reg['subredditMachineLearning'] = train_reg['subredditMachineLearning'].replace(1, 3)
train_reg['subredditmagicTCG'] = train_reg['subredditmagicTCG'].replace(1, 4)
train_reg['subredditpolitics'] = train_reg['subredditpolitics'].replace(1, 5)
train_reg['subredditReal_Estate'] = train_reg['subredditReal_Estate'].replace(1, 6)
train_reg['subredditscience'] = train_reg['subredditscience'].replace(1, 7)
train_reg['subredditStockMarket'] = train_reg['subredditStockMarket'].replace(1, 8)
train_reg['subreddittravel'] = train_reg['subreddittravel'].replace(1, 9)
train_reg['subredditvideogames'] = train_reg['subredditvideogames'].replace(1, 10)

# allows us to combine all of the dummy variable columns into one column 
train_labels = train_reg.replace(0,np.nan).max(1)

# saves the final train labels, final train dataframe, and final test dataframe to my google drive

train_labels.to_csv('/content/drive/My Drive/Stat_380/Final Project/final_train_labels.csv', index = False, header=True)

train_df.to_csv('/content/drive/My Drive/Stat_380/Final Project/final_train_data.csv', index = False, header=True)

test_df.to_csv('/content/drive/My Drive/Stat_380/Final Project/final_test_data.csv', index = False, header=True)

# defines the different ranges of parameters of XGBoost that we want to loop through
# pus them into a dictionary

params = dict()
params['max_depth'] = (2, 12)
params['learning_rate'] = (0.1, 1.0)
params['n_estimators'] = (100, 150)
params['booster'] = ('gbtree', 'gblinear', 'dart')

from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators = 120)

xgb.fit(train_df, train_labels)

prob_predictions = xgb.predict_proba(test_df)

# defines the scoring metric that we want to use for when we loop through the parameters

from sklearn.metrics import accuracy_score
acc_score = make_scorer(accuracy_score, greater_is_better = True)

# installs the scikit-optimize package that we will use for our bayesian optimization
pip install scikit-optimize

# uses bayesian optimization on the XGBoost model
# bayesian optimization focuses more on the parameters that are producing the best results and loops through
# those parameters while not focusing as much on the parameter values that are giving bad results
# we use k=7 cross fold validation to make the sure the parameters are producing the best results on different folds of data

from xgboost import XGBClassifier
from skopt import BayesSearchCV

xgb = XGBClassifier()

bayes_opt_xgb = BayesSearchCV(estimator = xgb, search_spaces = params, n_iter = 100, scoring = acc_score, cv=7)
bayes_opt_xgb.fit(train_df, train_labels)

# uses the bayes search model with the best hyperparameters to predict the probabilities of the test data
# we then save these predicitons as a variable
prob_predictions = bayes_opt_xgb.predict_proba(test_df)

# converts the probability predictions to a dataframe
prob_predictions = pd.DataFrame(prob_predictions)

# renames each of the columns to match the name of the sample submission files column names

df = prob_predictions.rename(columns={0: 'subredditcars', 1: 'subredditCooking', 2: 'subredditMachineLearning', 3: 'subredditmagicTCG', 4: 'subredditpolitics', 
                        5: 'subredditReal_Estate', 6:'subredditscience', 7: 'subredditStockMarket', 8: 'subreddittravel', 9: 'subredditvideogames' })

# inserts the id column from the sample submission into the predicted probability dataframe at index 0 (first column)
df.insert(0, 'id', samp_sub['id'])

# converts the predicted probability dataframe to a csv file to turn in on Kaggle

df.to_csv('/content/drive/My Drive/Stat_380/Final Project/submission17.csv', index = False, header=True)